{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8Q4Q3jona5kFLSdmQYC0d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hye0-n0/hye0-n0/blob/main/MLDL/%EB%8B%A4%EC%B8%B5_%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0_%EA%B5%AC%ED%98%84(mnist).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo6pjW46y0tZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y= True)\n",
        "\n",
        "y = y.astype(int)\n",
        "X = ((X/255) - 0.5) * 2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=123, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from neuralnet import NeuralNetMLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "Ywdy0GBKzgxP",
        "outputId": "e637ada7-19b5-4b45-8ff3-a2dcfb0b27cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4bbd4ce60019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneuralnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralNetMLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neuralnet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys"
      ],
      "metadata": {
        "id": "ZwGGFkxtzub0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetMLP(object):\n",
        "  \"\"\" 피드 포워드 신경망 / 다층 퍼셉트론 분류기\n",
        "\n",
        "  매개변수\n",
        "  -------------------------\n",
        "  n_hidden : int (기본값:30)\n",
        "    은닉 유닛 개수\n",
        "  l2 : float\n",
        "    L2 규제의 람다 값\n",
        "    l2 = 0 이면 규제 없음\n",
        "  epochs : int (기본값:100)\n",
        "      훈련 데이터셋을 반복할 횟수\n",
        "  eta : float (기본값:0.001)\n",
        "    학습률\n",
        "  shuffle : bool (기본값:True)\n",
        "    에포크마다 훈련 데이터셋을 섞을지 여부\n",
        "    True이면 데이터를 섞어 순서를 바꿈\n",
        "  minibatch_size : int (기본값:1)\n",
        "    미니 배치의 훈련 샘플 개수\n",
        "  seed : int\n",
        "    가중치와 데이터 셔플링을 위한 난수 초기값\n",
        "\n",
        "  속성\n",
        "  -------------------------\n",
        "  eval_ : dict\n",
        "    훈련 에포크마다 비용, 훈련 정확도, 검증 정확도를 수집하기 위한 딕셔너리\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_hidden=30, l2=0, epochs=100, eta=0.001, shuffle=True, minibatch_size=1, seed=None):\n",
        "    self.random = np.random.RandomState(seed)\n",
        "    self.n_hidden = n_hidden\n",
        "    self.l2 = l2\n",
        "    self.epochs = epochs\n",
        "    self.eta = eta\n",
        "    self.shuffle = shuffle\n",
        "    self.minibatch_size = minibatch_size\n",
        "\n",
        "  def _onehot(self, y, n_classes):\n",
        "    \"\"\" 레이블을 원-핫 방식으로 인코딩\n",
        "\n",
        "    매개변수\n",
        "    -----------------------\n",
        "    y : 배열, 크기=[n_samples]\n",
        "      타깃 값\n",
        "\n",
        "    반환값\n",
        "    -----------------------\n",
        "    onehot : 배열, 크기 = (n_samples, n_labels)\n",
        "    \"\"\"\n",
        "    onehot = np.zeros((n_classes, y.shape[0]))\n",
        "    for idx, val in enumerate(y.astype(int)):\n",
        "      onehot[val, idx] = 1\n",
        "      return onehot.T\n",
        "    \n",
        "  def _sigmoid(self, z):\n",
        "    \"\"\" 로지스틱 함수 계산 \"\"\"\n",
        "    return 1/ (1+ np.exp(np.clip(z, -250, 250)))\n",
        "\n",
        "  def _forward(self, X):\n",
        "    \"\"\" 정방향 계산 수행 \"\"\"\n",
        "    # 1단계 : 은닉층의 최종 입력\n",
        "    #[n_samples, n_features] dot [n_features, n_hidden]\n",
        "    #-> [n_samples, n_hidden]\n",
        "    z_h = np.dot(X, self.w_h) + self.b_h\n",
        "\n",
        "    # 2단계 : 은닉층 활성화 출력\n",
        "    a_h = self._sigmoid(z_h)\n",
        "\n",
        "    # 3단계 : 출력층의 최종 입력\n",
        "    #[n_samples, n_hidden] dot [n_hidden, n_calsslabels]\n",
        "    #-> [n_samples, n_classlabels]\n",
        "    z_out = np.dot(a_h, self.w_out) + self.b_out\n",
        "\n",
        "    # 4단계 : 출력층의 활성화 출력\n",
        "    a_out = self._sigmoid(z_out)\n",
        "\n",
        "    return z_h, a_h, z_out, a_out\n",
        "\n",
        "  def _compute_cost(self, y_enc, output):\n",
        "    \"\"\" 비용 함수 계산\n",
        "\n",
        "    매개변수\n",
        "    ----------------------\n",
        "    y_enc : 배열, 크기=(n_samples, n_labels)\n",
        "      원-핫 인코딩된 클래스 레이블\n",
        "    output : 배열, 크기=[n_samples, n_output_units]\n",
        "      출력층의 활성화 출력 (정방향 계산)\n",
        "\n",
        "    반환값\n",
        "    ----------------------\n",
        "    cost : float\n",
        "      규제가 포함된 비용\n",
        "    \"\"\"\n",
        "\n",
        "    L2_term = (self.l2 * (np.sum(self.w_h **2) + np.sum(self.w_out ** 2)))\n",
        "\n",
        "    term1 = -y_enc * (np.log(output))\n",
        "    term2 = (1 - y_enc) * np.log(1 - output)\n",
        "    cost = np.sum(term1 - term2) + L2_term\n",
        "    return cost\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\" 클래스 레이블을 예측\n",
        "\n",
        "    매개변수\n",
        "    ----------------------\n",
        "    X : 배열, 크기 = [n_samples, n_features]\n",
        "      원본 특성의 입력층\n",
        "\n",
        "    반환값\n",
        "    ----------------------\n",
        "    y_pred : 배열, 크기 = [n_samples]\n",
        "      예측된 클래스 레이블\n",
        "    \"\"\"\n",
        "    z_h, a_h, z_out, a_outt = self._forward(X)\n",
        "    y_pred = np.argmax(z_out, axis=1)\n",
        "    return y_pred\n",
        "\n",
        "  def fit(self, X_train, y_train, X_valid, y_valid):\n",
        "    \"\"\" 훈련 데이터에서 가중치를 학습\n",
        "\n",
        "    매개변수\n",
        "    ----------------------\n",
        "    X_train : 배열, 크기 = [n_samples, n_features]\n",
        "      원본 특성의 입력층\n",
        "    X_valid : 배열, 크기 = [n_samples]\n",
        "      타깃 클래스 레이블\n",
        "    y_train : 배열, 크기 = [n_samples, n_features]\n",
        "      훈련하는 동안 검증에 사용할 샘플 특성\n",
        "    y_valid : 배열, 크기 = [n_samples]\n",
        "      훈련하는 동안 검증에 사용할 샘플 레이블\n",
        "\n",
        "    반환값\n",
        "    ----------------------\n",
        "    self\n",
        "    \"\"\"\n",
        "\n",
        "    n_output = np.unique(y_train).shape[0] # 클래스 레이블 개수\n",
        "\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    ##########################\n",
        "    # 가중치 초기화\n",
        "    ##########################\n",
        "\n",
        "    # 입력층 -> 은닉층 사이의 가중치\n",
        "    self.b_h = np.zeros(self.n_hidden)\n",
        "    self.w_h = self.random.normal(loc=0, scale=0.1, size=(n_features, self.n_hidden))\n",
        "\n",
        "    # 은닉층 -> 출력층 사이의 가중치\n",
        "    self.b_out = np.zeros(n_output)\n",
        "    self.w_out = self.random.normal(loc=0, scale=0.1, size=(self.n_hidden, n_output))\n",
        "\n",
        "    epoch_strlen = len(str(self.epochs)) # 출력 포맷을 위해\n",
        "    self.eval_ = {'cost': [], 'train_acc' : [], 'valid_acc' : []}\n",
        "\n",
        "    y_train_enc = self._onehot(y_train, n_output)\n",
        "\n",
        "    # 훈련 에포크를 반복\n",
        "    for i in range(self.epochs):\n",
        "\n",
        "       # 미니 배치로 반복\n",
        "       indices = np.arange(X.train.shape[0])\n",
        "\n",
        "       if self.shuffle:\n",
        "         self.random.shuffle(indices)\n",
        "\n",
        "       for strat_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
        "         batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
        "         # 정방향 계산\n",
        "         z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
        "\n",
        "         ##########################\n",
        "         # 역전파\n",
        "         ##########################\n",
        "\n",
        "         # [n_samples, n_classlabels]\n",
        "         delta_out = a_out - y_train_enc[batch_idx]\n",
        "\n",
        "         # [n_samples, n_hidden]\n",
        "         sigmoid_derivative_h = a_h * (1 - a_h)\n",
        "\n",
        "         # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
        "         # -> [n_samples, n_hidden]\n",
        "         delta_h = (np.dot(delta_out, self.w_out.T) * sigmoid_derivative_h) \n",
        "\n",
        "         # [n_features, n_samples] dot [n_samples, n_hidden]\n",
        "         # -> [n_features, n_hidden]\n",
        "         grad_w_h = np.dot(X_train[batch_idx].T, delta_h)\n",
        "         grad_b_h = np.sum(delta_h, axis=0)\n",
        "\n",
        "         # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
        "         # -> [n_hidden, n_classlabels]\n",
        "         grad_w_out = np.dot(a_h.T, delta_out)\n",
        "         grad_b_out = np.sum(delta_out, axis=0)\n",
        "\n",
        "         # 규제와 가중치 업데이터\n",
        "         delta_w_h = (grad_w_h + self.l2*self.w_h)\n",
        "         delta_b_h = grad_b_h \n",
        "         self.w_h -= self.eta * delta_w_h\n",
        "         self.b_h -= self.eta * delta_b_h\n",
        "\n",
        "         delta_w_out = (grad_w_out + self.l2*self.w_out)\n",
        "         delta_b_out = grad_b_out\n",
        "         self.w_out -= self.eta * delta_w_out\n",
        "         self.b_out -= self.eta * delta_b_out\n",
        "\n",
        "       ##########################\n",
        "       # 평가\n",
        "       ##########################\n",
        "\n",
        "       # 훈련하는 동안 에포크마다 평가\n",
        "       z_h, a_h, z_out, a_out = self._forward(X_train)\n",
        "\n",
        "       cost = self._compute_cost(y_enc=y_train, output=a_out)\n",
        "  \n",
        "       y_train_pred = self.predict(X_train)\n",
        "       y_valid_pred = self.predict(X_valid)\n",
        "\n",
        "       train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float)) / X_train.shape[0]\n",
        "       valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float)) / X_valid.shape[0]\n",
        "\n",
        "       sys.stdfrr.write('\\r%0*d/%d | 비용 : %.2f'\n",
        "                      '| 훈련/검증 정확도 : %.2f%%/%.2f%%' % (epoch_strlen, i+1, self.epochs, cost, train_acc*100, valid_acc*100))\n",
        "       sys.stderr.fluch()\n",
        "\n",
        "       self.eval_['cost'].append(cost)\n",
        "       self.eval_['train_acc'].append(train_acc)\n",
        "       self.eval_['valid_acc'].append(valid_acc)\n",
        "\n",
        "    return self"
      ],
      "metadata": {
        "id": "fu56Yhtpz1NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetMLP(n_hidden=100, l2=0.01, epochs=200, eta=0.0005, minibatch_size=100, shuffle=True, seed=1)"
      ],
      "metadata": {
        "id": "E5fD248l7yfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.fit(X_train=X_train[:55000],\n",
        "       y_train=y_train[:55000],\n",
        "       X_valid=X_train[55000:],\n",
        "       y_valid=y_train[55000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "5mjDR4fBWMGH",
        "outputId": "5377ed9f-f7f9-4bb7-e3c1-2092c6254015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-6897c9f598d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m nn.fit(X_train=X_train[:55000],\n\u001b[0m\u001b[1;32m      2\u001b[0m        \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m55000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0mX_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m55000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m        y_valid=y_train[55000:])\n",
            "\u001b[0;32m<ipython-input-33-8ce10b242ef6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_valid, y_valid)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m        \u001b[0;31m# 미니 배치로 반복\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m        \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'train'"
          ]
        }
      ]
    }
  ]
}